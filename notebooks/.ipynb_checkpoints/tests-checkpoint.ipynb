{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e811ee-d040-4ff6-bf2b-caeaa2a65f0e",
   "metadata": {},
   "source": [
    "## Tests for runSparseHop parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd908cb2-e8c0-46b2-8b80-7465256841f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= -----TEST get_dlog!-----------------   \n",
    "\n",
    "produces three images\n",
    "grad-dL should be zero\n",
    "selected k should be the one with max dL \n",
    "k should be the same in trial and bisec\n",
    "k should be the one setting grad-dL to 0 and maximizing dL\n",
    "\n",
    "WORKS WELL\n",
    "*** I tested for pc and reg (not together) to (1e-3, 1e-2, 1e-1) =#\n",
    "\n",
    "TT = Float32; q = 21; L = 53; H = 21; reg = TT(1e-3); pc = TT(0); dL = TT.(zeros(L,L,H)); k = TT.(zeros(L,L,H)); y_k = TT.(zeros(L,L,H));\n",
    "f = 14; @time m = runSparseHop(V,   msa_file = seq_paths_dict[f], structfile = structs_dict[f], N_chains = 5000, \n",
    "                  N_iter = 0, \n",
    "                  pc = pc, \n",
    "                  reg = reg);\n",
    "get_dlog!(k, y_k, dL, V, m.D, m.M, m.str, reg, q, L, H)\n",
    "close(\"all\"); plt.scatter(k[:], y_k[:], label =\"k vs grad-dL\"); plt.legend(); savefig(\"../testSparseHop/test_bisection_pc$(pc)reg$(reg).png\") \n",
    "close(\"all\"); plt.scatter(k[:], dL[:], label =\"k vs dL\"); plt.legend(); savefig(\"../testSparseHop/test_dlog_pc$(pc)reg$(reg).png\")\n",
    "\n",
    "i1,i2,i3 = Tuple(argmax(dL)); opt_k = k[i1,i2,i3]; N_points = 100; acc = 0.01; trial_dL = []; trial_yk = []; trial_k = \n",
    "TT.([opt_k - (N_points*acc/2) + i*acc for i in 1:N_points]);\n",
    "for i in 1:N_points\n",
    "    push!(trial_dL, dlog(m.M.f2rs, m.D.f2rs, trial_k[i], V, i1, i2, i3, reg, q))\n",
    "    push!(trial_yk, zero_eq(trial_k[i], m.D.mheads[i1,i2,i3], m.M.f2rs, V, i1, i2, i3, reg, q))\n",
    "end\n",
    "close(\"all\"); plt.scatter(trial_k[:], trial_yk[:], label =\"k vs grad-dL\"); plt.scatter(\n",
    "    trial_k[:], trial_dL[:], label =\"k vs dL\"); plt.title(\"bisec $(round(opt_k, digits = 4)), trial $(round(trial_k[argmax(trial_dL[:])]\n",
    "    , digits = 4))\"); plt.legend(); savefig(\"../testSparseHop/bisec_vs_trial_pc$(pc)reg$(reg).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d204bad-1f32-4789-b465-b024c827a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= -----TEST run_gibbs_sampling! and update_ModelData! and grad_update!-----------------   \n",
    "@learn_rate = 1e-2\n",
    "\n",
    "-activate with bisection one edge to its optimal value\n",
    "-do 30 gradient updates separated by 5 gibbs sweeps each\n",
    "-see if k remains at the optimal value\n",
    "\n",
    "WORKS WELL WITHOUT FIELDS, DOES NOT WORK FOR PC = 0.1 OR LEARN_RATE = 0.1\n",
    "works a bit worse if you update both K[i,j,head] and K[j,i,head]\n",
    "\n",
    "*gradient update can be done also on fields, but not necessarily\n",
    " =#\n",
    "\n",
    "TT = Float32; grad_iter = 30; learn_r = TT(1e-2); q = 21; L = 53; opt_k = true; avoid_upd = false; H = \n",
    "21; reg = TT(1e-3); pc = TT(0); sweeps = 5; N_chains = 100000; dL = TT.(zeros(L,L,H)); k = TT.(zeros(L,L,H)); y_k = TT.(zeros(L,L,H));\n",
    "f = 14; @time m = runSparseHop(V, msa_file = seq_paths_dict[f], structfile = structs_dict[f], N_chains = N_chains, \n",
    "                  N_iter = 0, \n",
    "                  pc = pc, \n",
    "                  reg = reg,\n",
    "                  learn_r = learn_r);\n",
    "\n",
    "get_dlog!(k, y_k, dL, V, m.D, m.M, m.str, reg, q, L, H); i, j, head = Tuple(argmax(dL)); ks = []; grad_L = []; \n",
    "activate_edges!(k, m.K, y_k, dL, m.graf, m.full_graf, true, avoid_upd, opt_k, 1); m.K[i,j,head] = k[i,j,head];     \n",
    "\n",
    "for it in 1:grad_iter\n",
    "    run_gibbs_sampling!(m.chains, m.M.msa, m.h, m.J, m.K, V, L, m.full_graf, sweeps, N_chains)\n",
    "    update_ModelData!(m.M, V, L, pc, q, H)\n",
    "    push!(ks, m.K[i,j,head])\n",
    "    push!(grad_L, m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head])\n",
    "    println((m.K[i,j,head], m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head]))\n",
    "    m.h .+= learn_r .* (m.D.f1rs .- m.M.f1rs)\n",
    "    m.K[i, j, head] += learn_r * (m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2 * reg * m.K[i, j, head])\n",
    "    m.K[j, i, head] += learn_r * (m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2 * reg * m.K[i, j, head])\n",
    "    #grad_update!(m.h, m.K, m.D, m.M, m.graf, learn_r, reg, H)          \n",
    "end\n",
    "close(\"all\"); plt.plot(ks[:], label =\"iter vs k \"); plt.plot([k[i,j,head] for _ in 1:grad_iter], label = \"bisec opt_k\"); plt.title(\n",
    "    \"bisec $(round(k[i,j,head], digits = 4)), trial $(round(ks[argmin(abs.(grad_L[:]))]\n",
    "    , digits = 4))\");plt.legend(); savefig(\"../testSparseHop/test_gdesc_pc$(pc)reg$(reg).png\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25cc8d-16b4-4669-9897-044b67407c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test equilibration of K_ijh with h_i and h_j \n",
    "TT = Float32; grad_iter = 500; learn_r = TT(1); q = 21; L = 53; opt_k = true; avoid_upd = false; H = \n",
    "21; reg = TT(1e-1); pc = TT(0); sweeps = 5; N_chains = 100000; dL = TT.(zeros(L,L,H)); k = TT.(zeros(L,L,H)); y_k = TT.(zeros(L,L,H));\n",
    "f = 14; @time m = runSparseHop(V, msa_file = seq_paths_dict[f], structfile = structs_dict[f], N_chains = N_chains, \n",
    "    N_iter = 0, \n",
    "    pc = pc, \n",
    "    reg = reg,\n",
    "    learn_r = learn_r,\n",
    "    rand_init = false);\n",
    "\n",
    "get_dlog!(k, y_k, dL, V, m.D, m.M, m.str, reg, q, L, H); i, j, head = Tuple(argmax(dL)); ks = []; grad_L = []; \n",
    "activate_edges!(k, m.K, y_k, dL, m.graf, m.full_graf, true, avoid_upd, opt_k, 1); m.K[i,j,head] = k[i,j,head]; m.K[j,i,head] = m.K[i,j,head];     \n",
    "\n",
    "for it in 1:grad_iter\n",
    "    run_gibbs_sampling!(m.chains, m.M.msa, m.h, m.J, m.K, V, L, m.full_graf, sweeps, N_chains)\n",
    "    update_ModelData!(m.M, V, L, pc, q, H)\n",
    "    push!(ks, m.K[i,j,head])\n",
    "    push!(grad_L, m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head])\n",
    "    if it %20 == 0\n",
    "        println((m.K[i,j,head], m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head]))\n",
    "        @views println((i, sum(abs2, m.D.f1rs[:,i] .- m.M.f1rs[:,i])))\n",
    "        @views println((j, sum(abs2, m.D.f1rs[:,j] .- m.M.f1rs[:,j])))\n",
    "    end\n",
    "    m.h .+= learn_r .* (m.D.f1rs .- m.M.f1rs)\n",
    "    m.K[i, j, head] += learn_r * (m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2 * reg * m.K[i, j, head])\n",
    "    m.K[j, i, head] += learn_r * (m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2 * reg * m.K[i, j, head])          \n",
    "end\n",
    "\n",
    "close(\"all\"); plt.plot(ks[:], label =\"iter vs k \"); plt.plot([k[i,j,head] for _ in 1:grad_iter], label = \"bisec opt_k\"); plt.title(\n",
    "    \"final_grad mean = $(mean(grad_L[end-50:end])) var = $(std(grad_L[end-50:end]))\");plt.legend(); savefig(\"../testSparseHop/test_gdesc_pc$(pc)reg$(reg).png\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c760b3-ba59-4f3f-823a-d0cd85f184f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activating at optimal without fields\n",
    "TT = Float32; grad_iter = 500; learn_r = TT(1); q = 21; L = 53; opt_k = true; avoid_upd = false; H = \n",
    "21; reg = TT(1e-1); pc = TT(0); sweeps = 5; N_chains = 100000; dL = TT.(zeros(L,L,H)); k = TT.(zeros(L,L,H)); y_k = TT.(zeros(L,L,H));\n",
    "f = 14; @time m = runSparseHop(V, msa_file = seq_paths_dict[f], structfile = structs_dict[f], N_chains = N_chains, \n",
    "    N_iter = 0, \n",
    "    pc = pc, \n",
    "    reg = reg,\n",
    "    learn_r = learn_r,\n",
    "    rand_init = false);\n",
    "\n",
    "get_dlog!(k, y_k, dL, V, m.D, m.M, m.str, reg, q, L, H); i, j, head = Tuple(argmax(dL)); ks = []; grad_L = []; \n",
    "activate_edges!(k, m.K, y_k, dL, m.graf, m.full_graf, true, avoid_upd, opt_k, 1); m.K[i,j,head] = k[i,j,head]; m.K[j,i,head] = m.K[i,j,head];     \n",
    "\n",
    "for it in 1:grad_iter\n",
    "    run_gibbs_sampling!(m.chains, m.M.msa, m.h, m.J, m.K, V, L, m.full_graf, sweeps, N_chains)\n",
    "    update_ModelData!(m.M, V, L, pc, q, H)\n",
    "    push!(ks, m.K[i,j,head])\n",
    "    push!(grad_L, m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head])\n",
    "    if it %20 == 0\n",
    "        println((m.K[i,j,head], m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head]))\n",
    "        @views println((i, sum(abs2, m.D.f1rs[:,i] .- m.M.f1rs[:,i])))\n",
    "        @views println((j, sum(abs2, m.D.f1rs[:,j] .- m.M.f1rs[:,j])))\n",
    "    end\n",
    "    #m.h .+= learn_r .* (m.D.f1rs .- m.M.f1rs)\n",
    "    m.K[i, j, head] += learn_r * (m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2 * reg * m.K[i, j, head])\n",
    "    m.K[j, i, head] += learn_r * (m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2 * reg * m.K[i, j, head])          \n",
    "end\n",
    "\n",
    "close(\"all\"); plt.plot(ks[:], label =\"iter vs k \"); plt.plot([k[i,j,head] for _ in 1:grad_iter], label = \"bisec opt_k\"); plt.title(\n",
    "    \"final_grad mean = $(mean(grad_L[end-50:end])) var = $(std(grad_L[end-50:end]))\");plt.legend(); savefig(\"../testSparseHop/only_fields_test_gdesc_pc$(pc)reg$(reg).png\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32819875-45f8-40a8-b2e6-54cbb004d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test equilibration of K_ijh without updating h_i and h_j\n",
    "TT = Float32; grad_iter = 500; learn_r = TT(1); q = 21; L = 53; opt_k = true; avoid_upd = false; H = \n",
    "21; reg = TT(1e-1); pc = TT(0); sweeps = 5; N_chains = 100000; dL = TT.(zeros(L,L,H)); k = TT.(zeros(L,L,H)); y_k = TT.(zeros(L,L,H));\n",
    "f = 14; @time m = runSparseHop(V, msa_file = seq_paths_dict[f], structfile = structs_dict[f], N_chains = N_chains, \n",
    "    N_iter = 0, \n",
    "    pc = pc, \n",
    "    reg = reg,\n",
    "    learn_r = learn_r);\n",
    "\n",
    "get_dlog!(k, y_k, dL, V, m.D, m.M, m.str, reg, q, L, H); i, j, head = Tuple(argmax(dL)); ks = []; grad_L = []; \n",
    "activate_edges!(k, m.K, y_k, dL, m.graf, m.full_graf, true, avoid_upd, opt_k, 1); m.K[i,j,head] = 0; m.K[j,i,head] = m.K[i,j,head];     \n",
    "\n",
    "for it in 1:grad_iter\n",
    "    run_gibbs_sampling!(m.chains, m.M.msa, m.h, m.J, m.K, V, L, m.full_graf, sweeps, N_chains)\n",
    "    update_ModelData!(m.M, V, L, pc, q, H)\n",
    "    push!(ks, m.K[i,j,head])\n",
    "    push!(grad_L, m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head])\n",
    "    if it %20 == 0\n",
    "        println((m.K[i,j,head], m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head]))\n",
    "        @views println((i, sum(abs2, m.D.f1rs[:,i] .- m.M.f1rs[:,i])))\n",
    "        @views println((j, sum(abs2, m.D.f1rs[:,j] .- m.M.f1rs[:,j])))\n",
    "    end\n",
    "    #m.h .+= learn_r .* (m.D.f1rs .- m.M.f1rs)\n",
    "    m.K[i, j, head] += learn_r * (m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2 * reg * m.K[i, j, head])\n",
    "    m.K[j, i, head] += learn_r * (m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2 * reg * m.K[i, j, head])          \n",
    "end\n",
    "\n",
    "close(\"all\"); plt.plot(ks[:], label =\"iter vs k \"); plt.plot([k[i,j,head] for _ in 1:grad_iter], label = \"bisec opt_k\"); plt.title(\n",
    "    \"final_grad mean = $(mean(grad_L[end-50:end])) var = $(std(grad_L[end-50:end]))\");plt.legend(); savefig(\"../testSparseHop/only_fields_to_0_test_gdesc_pc$(pc)reg$(reg).png\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1d5e2-2fde-4598-8946-7b8d9617a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test equilibration of K_ijh with h_i and h_j on the 2nd coupling\n",
    "\n",
    "TT = Float32; grad_iter = 1000; learn_r = TT(1); q = 21; L = 53; opt_k = true; avoid_upd = false; H = \n",
    "21; reg = TT(1e-2); pc = TT(0); sweeps = 5; N_chains = 100000; dL = TT.(zeros(L,L,H)); k = TT.(zeros(L,L,H)); y_k = TT.(zeros(L,L,H));\n",
    "f = 14; @time m = runSparseHop(V, msa_file = seq_paths_dict[f], structfile = structs_dict[f], N_chains = N_chains, \n",
    "    N_iter = 1,\n",
    "    grad_iter = grad_iter,\n",
    "    pc = pc,\n",
    "    reg = reg,\n",
    "    n_edges = 1,\n",
    "    learn_r = learn_r,\n",
    "    rand_init = true);\n",
    "\n",
    "get_dlog!(k, y_k, dL, V, m.D, m.M, m.str, reg, q, L, H); i, j, head = Tuple(argmax(dL)); ks = []; grad_L = []; \n",
    "activate_edges!(k, m.K, y_k, dL, m.graf, m.full_graf, true, avoid_upd, opt_k, 1); m.K[i,j,head] = k[i,j,head]; m.K[j,i,head] = m.K[i,j,head];     \n",
    "\n",
    "for it in 1:grad_iter\n",
    "    run_gibbs_sampling!(m.chains, m.M.msa, m.h, m.J, m.K, V, L, m.full_graf, sweeps, N_chains)\n",
    "    update_ModelData!(m.M, V, L, pc, q, H)\n",
    "    push!(ks, m.K[i,j,head])\n",
    "    push!(grad_L, m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head])\n",
    "    grad_update!(m.h, m.K, m.D, m.M, m.graf, learn_r, reg, H)\n",
    "    if it %20 == 0\n",
    "        println((m.K[i,j,head], m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head]))\n",
    "        @views println((i, sum(abs2, m.D.f1rs[:,i] .- m.M.f1rs[:,i])))\n",
    "        @views println((j, sum(abs2, m.D.f1rs[:,j] .- m.M.f1rs[:,j])))\n",
    "    end\n",
    "              \n",
    "end\n",
    "close(\"all\"); plt.plot(ks[:], label =\"iter vs k \"); plt.plot([k[i,j,head] for _ in 1:grad_iter], label = \"bisec opt_k\"); plt.title(\n",
    "    \"final_grad mean = $(mean(grad_L[end-50:end])) var = $(std(grad_L[end-50:end]))\");plt.legend(); savefig(\"../testSparseHop/sub_test_gdesc_pc$(pc)reg$(reg).png\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1617cb-323d-4922-a998-b720727871d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= -----TEST run_gibbs_sampling! and update_ModelData! -----------------   \n",
    "\n",
    "-activate with bisection one edge to 0\n",
    "-try to see the grad-L as a function of some k values \n",
    "near the optimal k value suggested by the edge activation\n",
    "-bisec should be close to trial (maybe better looking at fist change in sign to determine trial k, \n",
    "here we use smaller value)\n",
    "\n",
    "WORKS WELL WITH EXCEPTION OF PC = 0.1\n",
    "N-chains at least 5* 10^3, need to work on entire matrix K\n",
    "if you upload Kij and not Kji doesn't work\n",
    "*** I tested for pc and reg (not together) to (1e-4, 1e-3, 1e-2, 1e-1) \n",
    " =#\n",
    "\n",
    "TT = Float32; grad_iter = 40; learn_r = TT(1e-1); q = 21; L = 53; opt_k = true; avoid_upd = false; H = \n",
    "21; reg = TT(1e-4); pc = TT(0); sweeps = 5; N_chains = 100000; dL = TT.(zeros(L,L,H)); k = TT.(zeros(L,L,H)); y_k = TT.(zeros(L,L,H));\n",
    "f = 14; acc = 0.01; @time m = runSparseHop(V,   msa_file = seq_paths_dict[f], structfile = structs_dict[f], N_chains = N_chains, \n",
    "                  N_iter = 0, \n",
    "                  pc = pc, \n",
    "                  reg = reg,\n",
    "                  learn_r = learn_r);\n",
    "\n",
    "get_dlog!(k, y_k, dL, V, m.D, m.M, m.str, reg, q, L, H); i, j, head = Tuple(argmax(dL)); ks = []; grad_L = []; \n",
    "activate_edges!(k, m.K, y_k, dL, m.graf, m.full_graf, true, avoid_upd, opt_k, 1); m.K[i,j,head] = k[i,j,head] - (acc*grad_iter/2);     \n",
    "\n",
    "for it in 1:grad_iter\n",
    "    run_gibbs_sampling!(m.chains, m.M.msa, m.h, m.J, m.K, V, L, m.full_graf, sweeps, N_chains)\n",
    "    update_ModelData!(m.M, V, L, pc, q, H)\n",
    "    push!(ks, m.K[i,j,head])\n",
    "    push!(grad_L, m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head])\n",
    "    println((m.K[i,j,head], m.D.mheads[i,j,head] - m.M.mheads[i,j,head] - 2*reg*m.K[i,j,head]))\n",
    "    m.K[i,j,head] += acc\n",
    "end\n",
    "close(\"all\"); plt.scatter(ks[:], grad_L[:], label =\"k vs trial grad-L\"); plt.title(\n",
    "    \"bisec $(round(k[i,j,head], digits = 4)), trial $(round(ks[argmin(abs.(grad_L[:]))]\n",
    "    , digits = 4))\");plt.legend(); savefig(\"../testSparseHop/test_grad_pc$(pc)reg$(reg).png\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0d1db-87d8-4077-8b19-fc4109f05519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= -----TEST run_gibbs_sampling! and update_ModelData! -----------------   \n",
    "\n",
    "\n",
    " =#\n",
    "\n",
    "TT = Float32; grad_iter = 40; learn_r = TT(1e-1); q = 21; L = 53; opt_k = true; avoid_upd = false; H = \n",
    "21; reg = TT(0); pc = TT(1e-2); sweeps = 5; N_chains = 100000; dL = TT.(zeros(L,L,H)); k = TT.(zeros(L,L,H)); y_k = TT.(zeros(L,L,H));\n",
    "f = 14; @time m = runSparseHop(V,   msa_file = seq_paths_dict[f], structfile = structs_dict[f], N_chains = N_chains, \n",
    "                  N_iter = 0, \n",
    "                  pc = pc, \n",
    "                  reg = reg,\n",
    "                  learn_r = learn_r);\n",
    "\n",
    "get_dlog!(k, y_k, dL, V, m.D, m.M, m.str, reg, q, L, H); i, j, head = Tuple(argmax(dL)); ks = []; grad_L = []; \n",
    "activate_edges!(k, m.K, y_k, dL, m.graf, m.full_graf, true, avoid_upd, opt_k, 1); m.K[i,j,head] = k[i,j,head];     \n",
    "run_gibbs_sampling!(m.chains, m.M.msa, m.h, m.J, m.K, V, L, m.full_graf, sweeps, N_chains)\n",
    "update_ModelData!(m.M, V, L, pc, q, H)\n",
    "println(m.D.mheads[i,j,head])\n",
    "println(m.M.mheads[i,j,head])\n",
    "println(2*reg*m.K[i,j,head])\n",
    "println(m.D.mheads[i,j,head]-m.M.mheads[i,j,head]-2*reg*m.K[i,j,head])\n",
    "println(m.K[i,j,head])\n",
    "println(i,j,head)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617b374-6835-4def-95e3-3af2617abe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=------------RESUME--------------\n",
    "\n",
    "- edge activation (get_dlog!) works fine in all scale\n",
    "\n",
    "- gibbs_sampling! and update_ModelData! works fine\n",
    "\n",
    "- bisection gives same results of gibbs_sampling and gradient descent\n",
    "if we just try to get the gradient given a lot of values around \n",
    "the optimal one (not true for pc = 1e-1, not true if we work \n",
    "only on upper diag of k, not true if chains<10^5, fluctuations are high)\n",
    "\n",
    "- if we initialize k in optimal value, gradient descent doesn't change too much\n",
    "still it's better to not update also the fields at every step \n",
    "(not true if chains<10^5, fluctuations are high)\n",
    "\n",
    "- if we initialize k a bit above or below optimal value, gradient descent is too slow,\n",
    "it doesn't rich the optimal value in 100 updates, i think this is due to fluctuations\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bada07f-da47-41d5-9d79-d177273c6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=------TEST VELOCITY GIBBS SAMPLING-----------\n",
    "=#\n",
    "\n",
    "TT = Float32; grad_iter = 40; learn_r = TT(1e-1); q = 21; L = 53; opt_k = true; avoid_upd = false; H = \n",
    "21; reg = TT(0); pc = TT(1e-2); sweeps = 5; N_chains = 10000; f = 14; @time m = runSparseHop(V,\n",
    "    msa_file = seq_paths_dict[f], \n",
    "    structfile = structs_dict[f], \n",
    "    N_chains = N_chains,                 \n",
    "    N_iter = 0, \n",
    "    pc = pc);\n",
    "\n",
    "@btime SparseHop.prob_cond!($m.chains[1], 1,$m.h,$m.J,53,$m.full_graf)\n",
    "@btime SparseHop.gibbs_sampling!($m.chains[1], $m.h,$m.J,53,$m.full_graf,5)\n",
    "@btime SparseHop.run_gibbs_sampling!($m.chains, $m.M.msa, $m.h, $m.J, $m.K, $V,$L, $m.full_graf, 5, N_chains)\n",
    "\n",
    "\n",
    "@btime SparseHop.new_prob_cond!($m.chains[1], 1,$m.h,$m.K,$V,53,21,$m.graf)\n",
    "@btime SparseHop.new_gibbs_sampling!($m.chains[1], $m.h,$m.K,$V,53,$m.graf,5,21)\n",
    "@btime SparseHop.new_run_gibbs_sampling!($m.chains, $m.M.msa, $m.h,$m.K,$V,$L, $m.graf, 5, N_chains, 21)\n",
    "\n",
    "SparseHop.prob_cond!(m.chains[1], 1,m.h,m.J,53,m.full_graf) - SparseHop.new_prob_cond!(m.chains[1], 1,m.h,m.K,V,53,21,m.graf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
